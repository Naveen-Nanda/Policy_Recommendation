# -*- coding: utf-8 -*-
"""ndvi_modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QIyaJpeJj2Q0NvrLeDLsKIXJz0T-enwM
"""

from google.colab import drive
drive.mount('/content/gdrive')

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor

os.chdir('/content/gdrive/MyDrive/Agri_Policy_Proj')

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")

!pip install joblib

!pip install einops

!python vision_transformer.py

import joblib
from vision_transformer import VisionTransformer

def min_max_normalization(input_array):
    min = input_array.min()
    max = input_array.max()
    return (input_array - min) / (max - min)

def load_ndvi_yield_data(pickle_path):
    try:
        # Load the pickled dictionary
        with open(pickle_path, 'rb') as f:
            ndvi_yield_preprocessed = pickle.load(f)
            ndvi = ndvi_yield_preprocessed["NDVI_Array"]
            yields = ndvi_yield_preprocessed["Yield"]
            ndvi_new = np.expand_dims(ndvi, axis=1)
            ndvi_normalized = min_max_normalization(ndvi_new)
        return ndvi_yield_preprocessed, ndvi_normalized, yields
    except FileNotFoundError:
        print(f"Error: File '{pickle_path}' not found.")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

pickle_path = '/content/gdrive/MyDrive/Agri_Policy_Proj/ndvi_yield_all.pkl'
ndvi_yield_preprocessed, ndvi_normalized, yields = load_ndvi_yield_data(pickle_path)

ndvi_normalized.shape

# Define a custom dataset
class CustomDataset(Dataset):
  def __init__(self, data):
      self.data = data

  def __len__(self):
      return len(self.data)

  def __getitem__(self, idx):
      return self.data[idx]

"""### RESNET50 + PCA + Random Forrest"""

def resnet50_features(input_array, batch_size):
    # Instantiate ResNet-50 model
    resnet = models.resnet50(weights=None)  # Load without pretrained weights
    resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    resnet.fc = nn.Identity()  # Remove the final fully connected layer

    # Assuming you have defined 'device' somewhere in your code
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    resnet.to(device)

    num_batches = len(input_array) // batch_size

    # Define the dataset and data loader
    dataset = CustomDataset(input_array)
    data_loader = DataLoader(dataset, batch_size=batch_size)

    # List to store concatenated feature vectors from all batches
    all_features = []

    # Set the model to evaluation mode
    resnet.eval()

    # Iterate over the data loader
    for batch_data in data_loader:
        # Pass the batch through ResNet-50
        with torch.no_grad():
            batch_features = resnet(batch_data.to(device, dtype=torch.float32))

        # Append the feature vectors for the current batch to the list
        all_features.append(batch_features)

    # Concatenate the feature vectors from all batches along the batch dimension
    concatenated_features = torch.cat(all_features, dim=0)
    feature_vectors = concatenated_features.detach().cpu().numpy()
    return feature_vectors

input_array = ndvi_normalized
batch_size = 32
feature_vectors = resnet50_features(input_array, batch_size)
print("Extracted features shape:", feature_vectors.shape)

def train_and_evaluate_rf(features, yields, test_size=0.2, components=100):
    # Perform PCA
    pca = PCA(n_components=components)
    reduced_data = pca.fit_transform(features)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(reduced_data, yields, test_size=test_size, random_state=24)

    # Initialize Random Forest regressor
    rf_regressor = RandomForestRegressor()

    # Train the model
    rf_regressor.fit(X_train, y_train)

    # Predict on the test set
    y_pred = rf_regressor.predict(X_test)

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test, y_pred)
    print("Mean Squared Error:", mse)

    # Plot actual vs. predicted values
    plt.figure(figsize=(10, 6))
    plt.plot(y_test, label='Actual', color='blue')
    plt.plot(y_pred, label='Predicted', color='red')
    plt.title(f'Actual vs. Predicted Values for {components} components')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Return trained model
    return rf_regressor

def save_trained_model(trained_model, model_filename, model_directory):
    try:
        # Construct the full path to the model file
        model_file_path = f"{model_directory}/{model_filename}"

        # Save the trained model
        joblib.dump(trained_model, model_file_path)
        print(f"Model saved successfully at: {model_file_path}")
    except Exception as e:
        print(f"An error occurred while saving the model: {e}")

trained_model = train_and_evaluate_rf(feature_vectors, yields)

model_filename = 'trained_rf_model.pkl'
model_directory = '/content/gdrive/MyDrive/Agri_Policy_Proj'
save_trained_model(trained_model, model_filename, model_directory)

def pca_model_fit(inp_feature_vecs, yields, model_file_path,components=100):
    # Load trained model
    trained_model = joblib.load(model_file_path)

    # Perform PCA
    pca = PCA(n_components=components)
    reduced_data = pca.fit_transform(inp_feature_vecs)

    # Predict on the test set
    yield_pred = trained_model.predict(reduced_data)

    # Calculate Mean Squared Error
    mse = mean_squared_error(yields, yield_pred)

    return yield_pred, mse

def visualize_predictions(y_pred, target):
    # Plot actual vs. predicted values
    plt.figure(figsize=(10, 6))
    plt.plot(target, label='Actual', color='blue')
    plt.plot(y_pred, label='Predicted', color='red')
    plt.title(f'Actual vs. Predicted Values for 100 components')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.show()



"""## ViT + PCA + XGBoost"""

def ViT_features(input_array, yields, batch_size):

    # Define your model (replace 'vit' with your actual model)
    vit = VisionTransformer(in_channels=1, emb_dim=128, patch_size=8, num_heads=2,
                        trx_ff_dim=512, num_trx_cells=2, num_class=1, hidden_dims=[1024,512])
    vit.to(device)

    inp_data = torch.tensor(input_array, dtype=torch.float32).to(device)
    custom_dataset = CustomDataset(inp_data)

    # Create a DataLoader for batching
    data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)

    # Initialize an empty list to store the outputs of each batch
    outputs = []

    # Iterate over the batches
    for batch_data in data_loader:
        # Pass the batch through your model
        with torch.no_grad():
            vit_features, vit_out = vit(batch_data)

        # Append the output of the batch to the list
        outputs.append(vit_features)

    # Concatenate the output tensors along the batch dimension
    concatenated_output = torch.cat(outputs, dim=0)

    return concatenated_output.detach().cpu().numpy()

input_array = ndvi_normalized
yields_data = yields
batch_size = 16
vit_features = ViT_features(input_array, yields_data, batch_size)
print(vit_features.shape)

def train_and_evaluate_XGB(features, yields, test_size=0.2, components=1000):
    # Perform PCA
    pca = PCA(n_components=components)
    reduced_data = pca.fit_transform(features)

    # Split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(reduced_data, yields, test_size=test_size, random_state=24)

    # Initialize XGBoost regressor
    xgb_regressor = XGBRegressor()

    # Train the model
    xgb_regressor.fit(X_train, y_train)

    # Predict on the test set
    y_pred = xgb_regressor.predict(X_test)

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test, y_pred)
    print("Mean Squared Error:", mse)

    # Plot actual vs. predicted values
    plt.figure(figsize=(10, 6))
    plt.plot(y_test, label='Actual', color='blue')
    plt.plot(y_pred, label='Predicted', color='red')
    plt.title(f'Actual vs. Predicted Values for {components} components')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Return trained model
    return xgb_regressor

trained_model = train_and_evaluate_XGB(vit_features, yields, components=1000)

model_filename = 'trained_vit_rf_model.pkl'
model_directory = '/content/gdrive/MyDrive/Agri_Policy_Proj'
save_trained_model(trained_model, model_filename, model_directory)

